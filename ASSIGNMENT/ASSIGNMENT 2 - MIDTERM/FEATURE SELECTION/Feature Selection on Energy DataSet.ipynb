{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION\n",
    "### 1. RECURSIVE FEATURE SELECTION\n",
    "### 2. FEATURE IMPORTANCE (TREE BASED)\n",
    "### 3. SELECT KBEST ( UNIVARIATE)\n",
    "### 4. SELECT FROM MODEL (L1)\n",
    "### 5. LOW VARIANCE (BASED ON SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import sklearn \n",
    "from sklearn import preprocessing \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from matplotlib import pyplot\n",
    "#from xgboost import plot_importance\n",
    "#from sklearn.feature_selection import chi2\n",
    "#from sklearn.feature_selection import f_classif\n",
    "#from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['month'] , df['time'] , df['hour'] ,df['day'] , df['day_of_week'],df['Numerical_Week'] =  df['date'].dt.month , df['date'].dt.time , df['date'].dt.hour , df['date'].dt.day , df['date'].dt.weekday_name,df['date'].dt.weekday\n",
    "df['weekStatus'] = df['date'].dt.dayofweek\n",
    "df['WeekStatus'] = np.where(df['weekStatus'] < 5, 'Weekday', 'Weekend')\n",
    "#df['DateStr'] = df['date'].apply(lambda x: x.strftime('%Y%m%d%H%M'))\n",
    "#df.head(5)\n",
    "\n",
    "# converting the time to seconds , appending that column to the existing dataframe and then making the 'date' column as index to \n",
    "# the dataset.\n",
    "\n",
    "d = df.date[0:len(df.date)]\n",
    "data = []\n",
    "#print(data)\n",
    "for i in range (len(d)):\n",
    "    if (i==0):\n",
    "        a = 61200\n",
    "        data.append(a)\n",
    "    elif (i > 0 ):\n",
    "        a = a + 600\n",
    "        data.append(a)\n",
    "#print((data))\n",
    "#df['data_converted'] = data\n",
    "df1 = pd.DataFrame({'data_converted' : data})\n",
    "df['NSM'] = df1\n",
    "\n",
    "#df['data_converted'] = df.astype(int)\n",
    "#df.columns\n",
    "df = df.set_index('date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#df = df.drop(['date'] ,axis = 1)\n",
    "df=df.drop(['day_of_week'] ,axis = 1)\n",
    "df=df.drop(['WeekStatus'] ,axis = 1)\n",
    "df=df.drop(['time'] ,axis = 1)\n",
    "\n",
    "#df=df.drop(['data_converted'] ,axis = 1)\n",
    "\n",
    "#df = df.astype('float32')\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = df.Appliances.values\n",
    "\n",
    "\n",
    "df = df.drop(['Appliances'] , axis = 1)\n",
    "\n",
    "X = df.as_matrix()\n",
    "features = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RECURSIVE FEATURE ELIMINATION :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 'Numerical_Week'), (1.0, 'RH_1'), (1.0, 'RH_2'), (1.0, 'RH_3'), (1.0, 'RH_4'), (1.0, 'RH_7'), (1.0, 'RH_8'), (1.0, 'T1'), (1.0, 'T2'), (1.0, 'T3'), (1.0, 'T4'), (1.0, 'T6'), (1.0, 'T7'), (1.0, 'T8'), (1.0, 'T9'), (1.0, 'T_out'), (1.0, 'Tdewpoint'), (1.0, 'Windspeed'), (1.0, 'hour'), (1.0, 'lights'), (1.0, 'month'), (1.0, 'weekStatus'), (2.0, 'T5'), (3.0, 'RH_9'), (4.0, 'RH_out'), (5.0, 'Visibility'), (6.0, 'day'), (7.0, 'Press_mm_hg'), (8.0, 'RH_6'), (9.0, 'rv1'), (10.0, 'rv2'), (11.0, 'RH_5'), (12.0, 'NSM')]\n",
      "            \n",
      "0.169190645744\n"
     ]
    }
   ],
   "source": [
    "estimator = LinearRegression()\n",
    "selector = RFE(estimator , 22 , step=1 )\n",
    "selector = selector.fit(X,Y)\n",
    "#selector.support_\n",
    "ranking = selector.ranking_\n",
    "#coeff = selector.coef_\n",
    "#fi = selector.feature_importances_\n",
    "print (sorted(zip(map(float, ranking), features)))\n",
    "#print(sorted(zip(map(float, coeff), features)))\n",
    "#print(\"           \")\n",
    "#print(sorted(zip(map(float, fi), features)))\n",
    "#print('ranking')\n",
    "prediction = selector.predict(X)\n",
    "scores = r2_score(Y , prediction)\n",
    "print(\"            \")\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 'NSM'), (1.0, 'Press_mm_hg'), (1.0, 'RH_1'), (1.0, 'RH_2'), (1.0, 'RH_3'), (1.0, 'RH_4'), (1.0, 'RH_5'), (1.0, 'RH_6'), (1.0, 'RH_7'), (1.0, 'RH_8'), (1.0, 'RH_out'), (1.0, 'T2'), (1.0, 'T3'), (1.0, 'T5'), (1.0, 'T8'), (1.0, 'T_out'), (1.0, 'Tdewpoint'), (1.0, 'hour'), (1.0, 'lights'), (1.0, 'rv2'), (2.0, 'RH_9'), (3.0, 'Windspeed'), (4.0, 'T7'), (5.0, 'T4'), (6.0, 'T6'), (7.0, 'Visibility'), (8.0, 'T1'), (9.0, 'rv1'), (10.0, 'day'), (11.0, 'T9'), (12.0, 'weekStatus'), (13.0, 'Numerical_Week'), (14.0, 'month')]\n",
      "            \n",
      "0.918020954333\n"
     ]
    }
   ],
   "source": [
    "estimator = RandomForestRegressor()\n",
    "selector = RFE(estimator ,20, step=1 )\n",
    "selector = selector.fit(X,Y)\n",
    "#selector.support_\n",
    "ranking = selector.ranking_\n",
    "print (sorted(zip(map(float, ranking), features)))\n",
    "print('            ')\n",
    "prediction = selector.predict(X)\n",
    "scores = r2_score(Y , prediction)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. FEATURE IMPORTANCE(TREE BASED) - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.016246955592418733, 'month'), (0.018709158983822062, 'T9'), (0.0193705098715039, 'Numerical_Week'), (0.019643934750393395, 'weekStatus'), (0.021117532265162893, 'day'), (0.0219977219606682, 'rv2'), (0.02228638689572151, 'T7'), (0.022349244119737842, 'rv1'), (0.023915764574716714, 'Visibility'), (0.024266931987183212, 'T5'), (0.02448226921996136, 'NSM'), (0.024773608894046565, 'RH_7'), (0.02533353404456651, 'Tdewpoint'), (0.02550190523041148, 'T_out'), (0.02558887317395682, 'RH_4'), (0.025594506469101864, 'Press_mm_hg'), (0.025913936815239253, 'T4'), (0.02606494850108655, 'RH_6'), (0.02670825251714159, 'RH_9'), (0.026874839200705807, 'T1'), (0.027411203014314364, 'RH_8'), (0.027567196824910916, 'T2'), (0.028712534411888056, 'T6'), (0.029538612666353773, 'RH_5'), (0.029806073535463943, 'T8'), (0.03122967901481279, 'RH_1'), (0.031706556443908705, 'Windspeed'), (0.032881951008522654, 'RH_2'), (0.03353832680553693, 'RH_3'), (0.034845374865078445, 'RH_out'), (0.037251048968137954, 'lights'), (0.038945877335933264, 'T3'), (0.149824750037592, 'hour')]\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "trees = ExtraTreesRegressor(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "trees = trees.fit(X, Y)\n",
    "importances = trees.feature_importances_\n",
    "print(sorted(zip(map(float ,importances),features)))\n",
    "print('            ')\n",
    "prediction = trees.predict(X)\n",
    "#scores = oob_score(Y ,prediction)\n",
    "#print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SELECT KBEST(UNIVARIATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 30.        ,  19.89      ,  47.59666667, ...,  92.        ,\n",
       "          7.        ,  17.        ],\n",
       "       [ 30.        ,  19.89      ,  46.69333333, ...,  92.        ,\n",
       "          6.66666667,  17.        ],\n",
       "       [ 30.        ,  19.89      ,  46.3       , ...,  92.        ,\n",
       "          6.33333333,  17.        ],\n",
       "       ..., \n",
       "       [ 10.        ,  25.5       ,  46.59666667, ...,  56.33333333,\n",
       "          3.66666667,  17.        ],\n",
       "       [ 10.        ,  25.5       ,  46.99      , ...,  56.66666667,\n",
       "          3.83333333,  17.        ],\n",
       "       [ 10.        ,  25.5       ,  46.6       , ...,  57.        ,\n",
       "          4.        ,  18.        ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_variables = SelectKBest(f_regression,k=20)                        #f_classif , chi2 , f_regression\n",
    "best_variables.fit(X,Y)\n",
    "best_variables = best_variables.fit_transform(X,Y)\n",
    "#print(zip(map(float ,best_variables) ,features))\n",
    "best_variables\n",
    "#best_variables[:1]\n",
    "#print(i for i in (zip(map(float ,best_variables) ,features)))\n",
    "\n",
    "'''\n",
    "f_classif\n",
    "ANOVA F-value between label/feature for classification tasks.\n",
    "\n",
    "mutual_info_classif\n",
    "Mutual information for a discrete target.\n",
    "\n",
    "chi2\n",
    "Chi-squared stats of non-negative features for classification tasks.\n",
    "\n",
    "f_regression\n",
    "F-value between label/feature for regression tasks.\n",
    "\n",
    "mutual_info_regression\n",
    "Mutual information for a continuous target.\n",
    "\n",
    "SelectPercentile\n",
    "Select features based on percentile of the highest scores.\n",
    "\n",
    "SelectFpr\n",
    "Select features based on a false positive rate test.\n",
    "\n",
    "SelectFdr\n",
    "Select features based on an estimated false discovery rate.\n",
    "\n",
    "SelectFwe\n",
    "Select features based on family-wise error rate.\n",
    "\n",
    "GenericUnivariateSelect\n",
    "Univariate feature selector with configurable mode.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SELECT FROM MODEL(L1) - LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  1.92202561e+00,   3.75236632e+00,   1.45430896e+01,\n",
      "         1.80860820e+01,   1.31094332e+01,   2.71830133e+01,\n",
      "         4.12038495e+00,   2.20257987e+00,   1.01738977e+00,\n",
      "         4.34037649e-02,   4.29283515e-02,   6.86899286e+00,\n",
      "         4.34249006e-02,   6.29268266e-01,   1.50454528e+00,\n",
      "         7.08519289e+00,   3.73542439e+00,   9.06159302e+00,\n",
      "         6.37288879e-01,   7.31765826e+00,   1.42342007e-01,\n",
      "         2.43972724e-01,   1.48410757e+00,   1.55961968e-01,\n",
      "         2.40097934e+00,   4.27670800e-02,   2.10533855e-17,\n",
      "         8.86260781e+00,   9.84385321e-01,   5.58805364e-01,\n",
      "         1.59417650e+00,   2.04845823e-13,   8.27707964e-06]), Index(['lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5',\n",
      "       'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out',\n",
      "       'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1',\n",
      "       'rv2', 'month', 'hour', 'day', 'Numerical_Week', 'weekStatus', 'NSM'],\n",
      "      dtype='object'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell-pc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso(alpha=.05)\n",
    "lasso.fit(X, Y)\n",
    "Lasso = (np.abs(lasso.coef_), features)\n",
    "print (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LOW VARIANCE (BASED ON SCORE)\n",
    "#### not preferred beacuse it considers only the X variables and not Y. So genearlly used for unsupervised methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.00000000e+01,   1.98900000e+01,   4.75966667e+01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   6.12000000e+04],\n",
       "       [  3.00000000e+01,   1.98900000e+01,   4.66933333e+01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   6.18000000e+04],\n",
       "       [  3.00000000e+01,   1.98900000e+01,   4.63000000e+01, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   6.24000000e+04],\n",
       "       ..., \n",
       "       [  1.00000000e+01,   2.55000000e+01,   4.65966667e+01, ...,\n",
       "          4.00000000e+00,   4.00000000e+00,   1.19004000e+07],\n",
       "       [  1.00000000e+01,   2.55000000e+01,   4.69900000e+01, ...,\n",
       "          4.00000000e+00,   4.00000000e+00,   1.19010000e+07],\n",
       "       [  1.00000000e+01,   2.55000000e+01,   4.66000000e+01, ...,\n",
       "          4.00000000e+00,   4.00000000e+00,   1.19016000e+07]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_features = VarianceThreshold(threshold=(.2 * (1 - 0.0)))\n",
    "desired_features.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. Out of these methods only Recursive Feature Elimination ( Random Forest Regressor) has proved to be best. \n",
    "### 2. Others such SelectKBest , Based on variance are univariate models. \n",
    "### 3. L1 based feature selection doesn't provide good output.\n",
    "### 4. ExtraTreeRegressor is good , but the output when compared with other automated feature selection tools doesn't match.\n",
    "### 5. Recursive Feature Elimination(Random Forest) is selected over Recursive Feature Elimination (Linear Regression) because the variance of the former is very high compared to latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
